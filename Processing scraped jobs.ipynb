{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing scraped jobs\n",
    "\n",
    "Once job details and descriptions are scraped a few more things need to happen:\n",
    "\n",
    "* compare today's vs. yesterday's jobs\n",
    "* when new job comes in, add day tracker (days = 0)\n",
    "* if job not new, days += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import some libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# plotting pacakges\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly as py\n",
    "%matplotlib inline\n",
    "\n",
    "# ignore matplotlib/seaborn deprecation warning... it just looks ugly\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"The 'normed' kwarg is deprecated, and has been \")\n",
    "\n",
    "# assortment of other libraries\n",
    "import re\n",
    "import time\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import arrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   test1  test2\n",
      "0      3      6\n",
      "1      2      7\n",
      "2      1      4\n",
      "3      3      2\n",
      "4      1      1\n",
      "   test1  test2\n",
      "0      3      7\n",
      "1      2      2\n",
      "2      1      4\n",
      "3      3      3\n",
      "4      1      6\n",
      "   test1  test2\n",
      "0      3      7\n",
      "4      1      6\n",
      "2      1      4\n",
      "3      3      3\n",
      "1      2      2\n",
      "   test1  test2\n",
      "0      3      7\n",
      "4      1      6\n",
      "2      1      4\n",
      "3      3      3\n",
      "1      2      2\n"
     ]
    }
   ],
   "source": [
    "df1 = pd.DataFrame({\"test1\": [3,2,1,3,1], \"test2\": [6,7,4,2,1]})\n",
    "df2 = pd.DataFrame({\"test1\": [1,5,3,3,2]})\n",
    "print(df1)\n",
    "df3 = df2.merge(df1, on=['test1'], how=\"right\", indicator=True)\n",
    "\n",
    "df3.loc[df3['test2'].isnull(), 'test2'] = 0\n",
    "big = df3[df3['test2']>4].index\n",
    "add = [1,-5,0,1,5]\n",
    "df1['test2'] += add\n",
    "print(df1)\n",
    "df1 = df1.sort_values(by=['test2'], ascending=False)\n",
    "print(df1)\n",
    "print(df1.iloc[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INITIALIZE FIRST DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(\"./data/scrapes/raw_scrapes/glassdoor-df-06-06-2018.csv\", encoding=\"ISO-8859-1\")\n",
    "df_test2 = pd.read_csv(\"./data/scrapes/raw_scrapes/glassdoor-df-06-08-2018.csv\", encoding=\"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_first_df(df):\n",
    "\n",
    "    # get rid of duplicates\n",
    "    dups = df.duplicated(subset=['position','company','location','description'])\n",
    "    df = df[~dups]\n",
    "\n",
    "    # add some new columns of interest\n",
    "    df['days_posted'] = 0\n",
    "    df['date_posted'] = arrow.now().format('MM-DD-YYYY')\n",
    "    df['applied'] = \"No\"\n",
    "    df['date_applied'] = np.nan\n",
    "    df['strikes'] = 0\n",
    "    df['rank'] = 0\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\skarb\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\ipykernel_launcher.py:8: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "\n",
      "c:\\users\\skarb\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\ipykernel_launcher.py:9: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "\n",
      "c:\\users\\skarb\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\ipykernel_launcher.py:10: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "\n",
      "c:\\users\\skarb\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\ipykernel_launcher.py:11: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "\n",
      "c:\\users\\skarb\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\ipykernel_launcher.py:12: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "\n",
      "c:\\users\\skarb\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\ipykernel_launcher.py:13: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_test = initialize_first_df(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>position</th>\n",
       "      <th>company</th>\n",
       "      <th>location</th>\n",
       "      <th>description</th>\n",
       "      <th>link</th>\n",
       "      <th>days_posted</th>\n",
       "      <th>date_posted</th>\n",
       "      <th>applied</th>\n",
       "      <th>date_applied</th>\n",
       "      <th>strikes</th>\n",
       "      <th>rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Scientist - New Grad</td>\n",
       "      <td>Yelp</td>\n",
       "      <td>San Francisco, CA</td>\n",
       "      <td>At Yelp, we see our 148 million reviews not on...</td>\n",
       "      <td>https://www.glassdoor.com/partner/jobListing.h...</td>\n",
       "      <td>0</td>\n",
       "      <td>06-11-2018</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Scientist - New Grad</td>\n",
       "      <td>Quora, Inc.</td>\n",
       "      <td>Mountain View, CA</td>\n",
       "      <td>Because Quora is such a data-driven company, o...</td>\n",
       "      <td>https://www.glassdoor.com/partner/jobListing.h...</td>\n",
       "      <td>0</td>\n",
       "      <td>06-11-2018</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Machine Learning Scientist (University Grad)</td>\n",
       "      <td>Electronic Arts</td>\n",
       "      <td>Redwood City, CA</td>\n",
       "      <td>We are EA\\r\\n\\r\\nAnd we make games how cool is...</td>\n",
       "      <td>https://www.glassdoor.com/partner/jobListing.h...</td>\n",
       "      <td>0</td>\n",
       "      <td>06-11-2018</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>New Grad - Data Scientist</td>\n",
       "      <td>Viasat</td>\n",
       "      <td>Boston, MA</td>\n",
       "      <td>Job ResponsibilitiesBecome a personal contribu...</td>\n",
       "      <td>https://www.glassdoor.com/partner/jobListing.h...</td>\n",
       "      <td>0</td>\n",
       "      <td>06-11-2018</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       position          company  \\\n",
       "0                     Data Scientist - New Grad             Yelp   \n",
       "1                     Data Scientist - New Grad      Quora, Inc.   \n",
       "2  Machine Learning Scientist (University Grad)  Electronic Arts   \n",
       "3                     New Grad - Data Scientist           Viasat   \n",
       "\n",
       "            location                                        description  \\\n",
       "0  San Francisco, CA  At Yelp, we see our 148 million reviews not on...   \n",
       "1  Mountain View, CA  Because Quora is such a data-driven company, o...   \n",
       "2   Redwood City, CA  We are EA\\r\\n\\r\\nAnd we make games how cool is...   \n",
       "3         Boston, MA  Job ResponsibilitiesBecome a personal contribu...   \n",
       "\n",
       "                                                link  days_posted date_posted  \\\n",
       "0  https://www.glassdoor.com/partner/jobListing.h...            0  06-11-2018   \n",
       "1  https://www.glassdoor.com/partner/jobListing.h...            0  06-11-2018   \n",
       "2  https://www.glassdoor.com/partner/jobListing.h...            0  06-11-2018   \n",
       "3  https://www.glassdoor.com/partner/jobListing.h...            0  06-11-2018   \n",
       "\n",
       "  applied  date_applied  strikes  rank  \n",
       "0      No           NaN        0     0  \n",
       "1      No           NaN        0     0  \n",
       "2      No           NaN        0     0  \n",
       "3      No           NaN        0     0  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head(n=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     20,
     39
    ]
   },
   "outputs": [],
   "source": [
    "def filter_by_location(df):\n",
    "\n",
    "    filter_index = []\n",
    "    locations = df['location'].tolist()\n",
    "\n",
    "    for location in locations:\n",
    "        try:\n",
    "            state = location.split(',')[1].strip()\n",
    "            if state in self.locations_of_interest:\n",
    "                filter_index.append(True)\n",
    "            else:\n",
    "                filter_index.append(False)\n",
    "        except:\n",
    "            if location == 'Remote':\n",
    "                filter_index.append(True)\n",
    "            else:\n",
    "                filter_index.append(False)\n",
    "\n",
    "    return(df[filter_index])\n",
    "\n",
    "def filter_by_position_title(self):\n",
    "\n",
    "    filter_jobs = []\n",
    "    jobs = self.df_new['position'].tolist()\n",
    "\n",
    "    for job in jobs:\n",
    "        job_words = job.split(\" \")\n",
    "        for word in job_words:\n",
    "            if word.lower() in filter_words:\n",
    "                rank = 2\n",
    "                break;\n",
    "            else:\n",
    "                include = True\n",
    "        if include:\n",
    "            filter_jobs.append(True)\n",
    "        else:\n",
    "            filter_jobs.append(False)\n",
    "    df = df[filter_jobs]\n",
    "\n",
    "def filter_by_description(self):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_test = filter_by_location(df_test)\n",
    "# df_test = filter_by_position_title(df_test)\n",
    "# df_test = filter_by_description(df_test)\n",
    "df_test.to_csv(\"./data/scrapes/cleaned_scrapes/glassdoor-df-06-06-2018.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PROCESSOR CLASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "code_folding": [
     107
    ]
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import arrow\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "import re\n",
    "from collections import Counter\n",
    "import string\n",
    "import time\n",
    "import sys\n",
    "\n",
    "\n",
    "class process_raw_scrape():\n",
    "    \n",
    "    def __init__(self, old_df, df_new):\n",
    "    \n",
    "        self.df_new = df_new\n",
    "        self.df_old = old_df\n",
    "        self.archive_df = pd.DataFrame()\n",
    "        \n",
    "        # used for ranking system\n",
    "        self.locations_of_interest = ['ca', 'co', 'hi', 'me', 'mi', 'ny', 'oh', 'or', 'tx', 'wa'] # +2\n",
    "        self.words_of_interest = ['data'] # +1\n",
    "        self.filter_words = ['senior', 'sr.', 'sr', 'director', 'manager', 'research', 'architect'] # -1\n",
    "        self.key_skills = ['data science', 'python', 'statistical analysis']\n",
    "\n",
    "    # DONE #\n",
    "    def combine_dfs(self):\n",
    "        \n",
    "        start = time.time()\n",
    "        \n",
    "        print(\"Removing duplicates from today's web scrape.\")\n",
    "        #prepare df_new\n",
    "        dups = self.df_new.duplicated(subset=['position','company','location','description'])\n",
    "        self.df_new = self.df_new[~dups]\n",
    "\n",
    "        print(\"Merging today's web scrape with yesterday's.\")\n",
    "        # full outer join on old and new df (and get rid of duplicate columns)\n",
    "        self.df_new = self.df_new.merge(self.df_old, on=['company','position','location','description'], how=\"outer\", indicator=True)\n",
    "        self.df_new = self.df_new.drop(columns=['link_x'])\n",
    "        self.df_new = self.df_new.rename(columns= {'link_y':'link'})\n",
    "        self.df_new.loc[self.df_new['strikes'].isnull(), 'strikes'] = 0\n",
    "\n",
    "        # mark any aging jobs (but don't remove just yet)\n",
    "        self.df_new.loc[self.df_new['_merge']=='right_only', 'strikes'] += 1\n",
    "        \n",
    "        # save old jobs to archive\n",
    "        self.archive_df = self.df_new[self.df_new['strikes'] >= 3]\n",
    "        if not self.archive_df.empty:\n",
    "            self.df_new = self.df_new.drop(self.archive_df.index)\n",
    "            self.archive_df.to_csv(\"../data/scrapes/archived_scrapes/glassdoor-df-{}.csv\".format(arrow.now().format('MM-DD-YYYY')).csv, \n",
    "                                   index=False)\n",
    "        else:\n",
    "            print(\"No jobs old enough to archive today.\")\n",
    "\n",
    "        # days_posted\n",
    "        self.df_new.loc[self.df_new['_merge']=='both', 'days_posted'] += 1\n",
    "        self.df_new.loc[self.df_new['days_posted'].isnull(), 'days_posted'] = 0 \n",
    "\n",
    "        # date_posted\n",
    "        self.df_new.loc[self.df_new['date_posted'].isnull(), 'date_posted'] = arrow.now().format('MM-DD-YYYY') \n",
    "\n",
    "        # applied\n",
    "        self.df_new.loc[self.df_new['applied'].isnull(), 'applied'] = \"No\"   \n",
    "        \n",
    "        # rank\n",
    "        self.df_new.loc[self.df_new['rank'].isnull(), 'rank'] = 0\n",
    "\n",
    "        # drop merge\n",
    "        self.df_new = self.df_new.drop(columns=['_merge'])\n",
    "        \n",
    "        save_location = \"../data/scrapes/cleaned_scrapes/glassdoor-df-{}.csv\".format(arrow.now().format('MM-DD-YYYY'))\n",
    "        self.df_new.to_csv(save_location, index=False)\n",
    "        print(\"Cleaned web scrape saved to {}\".format(save_location))\n",
    "        \n",
    "        end = time.time()\n",
    "        \n",
    "        print(\"Merging dataframes complete!\")\n",
    "        print(\"Total time: {:.2f}\".format((end-start)/60))\n",
    "        print(\"\\n\")\n",
    "\n",
    "    # DONE #\n",
    "    def filter_by_location(self):\n",
    "\n",
    "        locations = self.df_new['location'].tolist()\n",
    "        ranks = [0]*len(locations)\n",
    "        \n",
    "        for i,location in enumerate(locations):\n",
    "            try:\n",
    "                if location == 'Remote':\n",
    "                    ranks[i] += 3\n",
    "                    continue;\n",
    "                state = location.split(',')[1].strip()\n",
    "                if state.lower() in self.locations_of_interest:\n",
    "                    ranks[i] += 3\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        self.df_new['rank'] += ranks\n",
    "\n",
    "    # DONE #\n",
    "    def filter_by_position_title(self):\n",
    "\n",
    "        jobs = self.df_new['position'].tolist()\n",
    "        ranks = [0]*len(jobs)\n",
    "        \n",
    "        for i,job in enumerate(jobs):\n",
    "            \n",
    "            job = job.lower()\n",
    "            \n",
    "            if (re.search(\"scientist\", job)) and not (re.search(\"data\", job)):\n",
    "                ranks[i] += -3\n",
    "            \n",
    "            first_plus = True\n",
    "            first_minus = True\n",
    "            \n",
    "            job_words = job.split(\" \")\n",
    "            for word in job_words:\n",
    "                if (first_plus) and (word in self.words_of_interest):\n",
    "                    first_plus = False\n",
    "                    ranks[i] += 1\n",
    "                elif (first_minus) and (word in self.filter_words):\n",
    "                    first_minus = False\n",
    "                    ranks[i] += -1\n",
    "                \n",
    "                \n",
    "        self.df_new['rank'] += ranks\n",
    "    \n",
    "    # DONE #\n",
    "    def filter_by_description(self):\n",
    "        \n",
    "        descriptions = self.df_new['descriptions'].tolist()\n",
    "        ranks = [0]*len(descriptions)\n",
    "        \n",
    "        for i,description in enumerate(descriptions):\n",
    "            \n",
    "            lowered = description.lower()\n",
    "            bachelor = True if re.search(\"bachelor|b\\.?s\\.?\", lowered) else False\n",
    "            master = True if re.search(\"master|ms|m\\.s\\.|ms\\.\", lowered) else False\n",
    "            phd = True if re.search(\"doctorate|p\\.?h\\.?d\\.?\", lowered) else False\n",
    "            experience_2 = True if re.search(\"2\\+?year[s]?\", lowered) else False\n",
    "            experience_5 = True if re.search(\"5\\+?year[s]?\", lowered) else False\n",
    "            experience_10 = True if re.search(\"10\\+?year[s]?\", lowered) else False\n",
    "            good_words = True if re.search(words_of_interest, lowered) else False\n",
    "\n",
    "            if (experience_5 and phd) or (phd and not master) or (experience_10):\n",
    "                ranks[i] += -2\n",
    "            elif (experience_2 and not phd and not master) or (experience_5 and not phd):\n",
    "                ranks[i] += -1\n",
    "            elif (master and not phd and not experience_10) or (bachelor):\n",
    "                ranks[i] += 2\n",
    "            if good_words:\n",
    "                ranks[i] += 2\n",
    "            \n",
    "        self.df_new['rank'] += ranks\n",
    "    \n",
    "    # DONE #\n",
    "    def save_top_jobs(self):\n",
    "        \n",
    "        start = time.time()\n",
    "        \n",
    "        print(\"Beginning to \")\n",
    "        top_jobs = Counter(self.df_new['position'].tolist()).most_common(20)\n",
    "        \n",
    "        save_location = '../data/app_data/current_plot_data/top-jobs-{}.txt'.format(arrow.now().format('MM-DD-YYYY'))\n",
    "        with open(save_location, 'w') as file:\n",
    "            for job in top_jobs:\n",
    "                if isinstance(job[0], float):\n",
    "                    continue;\n",
    "                elif (len(job[0]) > 45):\n",
    "                    continue;\n",
    "                else:\n",
    "                    file.write('{0};{1}'.format(job[0], job[1]))\n",
    "                    file.write('\\n')\n",
    "       \n",
    "        end = time.time()\n",
    "        print(\"Finished parsing top jobs. Saved to {}\".format(save_location))\n",
    "        print(\"Total time: {:.2f} minutes.\".format((end-start)/60))\n",
    "        print(\"\\n\")\n",
    "    \n",
    "    # DONE #\n",
    "    def save_top_terms(self):\n",
    "        \n",
    "        start = time.time()\n",
    "        \n",
    "        print(\"Beginning to parse descriptions for top terms.\")\n",
    "        descriptions = self.df_new['description'].tolist()\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        common_terms = []\n",
    "        \n",
    "        for description in descriptions:\n",
    "            \n",
    "            words = tester.strip().replace('\\n', ' ').translate(string.punctuation) # strip down to words\n",
    "            words = re.sub(r'[^\\w\\s]', '', words)\n",
    "\n",
    "            tokens = word_tokenize(words)\n",
    "            tokens = [word for word in tokens if not word in stop_words]\n",
    "\n",
    "            unigrams = list(ngrams(tokens, 1))\n",
    "            bigrams = list(ngrams(tokens, 2))\n",
    "            trigrams = list(ngrams(tokens, 3))\n",
    "\n",
    "            for unigram in unigrams:\n",
    "                common_terms.append(unigram[0])\n",
    "\n",
    "            for bigram in bigrams:\n",
    "                common_terms.append(\"{0} {1}\".format(bigram[0], bigram[1]))\n",
    "\n",
    "            for trigram in trigrams:\n",
    "                common_terms.append(\"{0} {1} {2}\".format(trigram[0], trigram[1], trigram[2]))\n",
    "\n",
    "                \n",
    "        top_terms = Counter(common_terms).most_common(20)\n",
    "\n",
    "        save_location = \"../data/app_data/current_plot_data/top-terms-{}.txt\".format(arrow.now().format('MM-DD-YYYY'))\n",
    "        with open(save_location, 'w') as file:\n",
    "            for word in counted_words:\n",
    "                try:\n",
    "                    file.write(\"{0};{1}\".format(word[0], word[1]))\n",
    "                    file.write(\"\\n\")\n",
    "                except:\n",
    "                    print(\"Couldn't write top term: '{0}' ({1}) to file\".format(word[0], type(word[0])))\n",
    "        end = time.time()\n",
    "        print(\"Finished parsing top terms. Saved to {}\".format(save_location))\n",
    "        print(\"Total time: {:.2f} minutes\".format((end-start)/60))\n",
    "        print(\"\\n\")\n",
    "        \n",
    "    # DONE #  \n",
    "    def save_df(self):\n",
    "        \n",
    "        # save cleaned_scrape to disk\n",
    "        self.df_new.to_csv(\"../data/scrapes/clean_filtered_scrapes/glassdoor-df-{}.csv\".format(arrow.now().format('MM-DD-YYYY')), index=False)\n",
    "        print(\"Saved cleaned and filtered web scrape.\")\n",
    "        \n",
    "        # save cleaned_scrape without descriptions for app upload\n",
    "        jobs_for_app = self.df_new.drop('description', axis=1)\n",
    "        jobs_for_app.to_csv(\"../data/app_data/jobs-for-app.csv\", index=False)\n",
    "        print(\"Saved jobs for web app.\")\n",
    "        \n",
    "    \n",
    "# main program to run from terminal/command-line\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    # get today's and yesterday's web scrapes ready\n",
    "    old_df = pd.read_csv(\"../data/scrapes/cleaned_scrapes/glassdoor-df-{}.csv\".format(arrow.now().shift(days=-1).format('MM-DD-YYYY')), \n",
    "                         encoding=\"ISO-8859-1\")\n",
    "    new_df = pd.read_csv(\"../data/scrapes/raw_scrapes/glassdoor-df-{}.csv\".format(arrow.now().format('MM-DD-YYYY')), \n",
    "                         encoding=\"ISO-8859-1\")\n",
    "   \n",
    "\n",
    "    ######################################\n",
    "    # STEP 1: initialize processor class #\n",
    "    ######################################\n",
    "    try:\n",
    "        job_processor = process_raw_scrape(old_df, new_df)\n",
    "    except:\n",
    "        print(\"Issue initializing processor class. Shutting down.\")\n",
    "        sys.exit()\n",
    "        \n",
    "        \n",
    "    ######################################\n",
    "    # STEP 2: compare yesterday to today #\n",
    "    ######################################\n",
    "    try:\n",
    "        job_processor.combine_dfs()\n",
    "    except:\n",
    "        print(\"Issue in combine_dfs(). Shutting down.\")\n",
    "        sys.exit()\n",
    "\n",
    "        \n",
    "    ###################################################\n",
    "    # STEP 3: set rankings and remove irrelevant jobs #\n",
    "    ###################################################\n",
    "    try:\n",
    "        job_processor.filter_by_location()\n",
    "    except:\n",
    "        print(\"Issue in filter_by_location(). Shutting down.\")\n",
    "        sys.exit()\n",
    "    try:\n",
    "        job_processor.filter_by_position_title()\n",
    "    except:\n",
    "        print(\"Issue in filter_by_position_title(). Shutting down.\")\n",
    "        sys.exit()\n",
    "    try:\n",
    "        job_processor.filter_by_description()\n",
    "    except:\n",
    "        print(\"Issue in filter_by_description(). Shutting down.\")\n",
    "        sys.exit()\n",
    "           \n",
    "            \n",
    "    #########################\n",
    "    # STEP 4: get plot data #\n",
    "    #########################\n",
    "    try:\n",
    "        job_processor.save_top_jobs()\n",
    "    except:\n",
    "        print(\"Issue in save_top_jobs(). Shutting down.\")\n",
    "        sys.exit()\n",
    "        \n",
    "    try:\n",
    "        job_processor.save_top_terms()\n",
    "    except:\n",
    "        print(\"Issue in save_top_terms(). Shutting down.\")\n",
    "        sys.exit()\n",
    "    \n",
    "    \n",
    "    ###########################\n",
    "    # STEP 5: save cleaned df #\n",
    "    ###########################\n",
    "    try:\n",
    "        job_processor.save_df()\n",
    "    except:\n",
    "        print(\"Issue in save_df(). Shutting down.\")\n",
    "        sys.exit()\n",
    "    \n",
    "    # we made it to the end!\n",
    "    print(\"RAW WEB SCRAPE FULLY PROCESSED!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "booo scientist bio\n",
      "Yaaay data scientist\n"
     ]
    }
   ],
   "source": [
    "practice = [\"Scientist bio\", \"Data Scientist\"]\n",
    "\n",
    "for title in practice:\n",
    "    title = title.lower()\n",
    "    \n",
    "    if (re.search(\"scientist\", title)) and not (re.search(\"data\", title)):\n",
    "        print(\"booo\", title)\n",
    "    else:\n",
    "        print(\"Yaaay\", title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "test = [0]*20\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4906, 11)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4906"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = df_test['description'].isnull().tolist()\n",
    "\n",
    "len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4906, 11)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(920, 11)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test[df_test['link'].isnull()].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3986, 11)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test[~df_test['link'].isnull()].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Filter by words in description**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main ideas:\n",
    "\n",
    "1) Rank jobs 1-3 (1 highest).\n",
    "\n",
    "\n",
    "2) Filter on some ideas.\n",
    "\n",
    "* if 5+ years experience: rank 3\n",
    "* if Phd and not Master's: rank 3\n",
    "* if no mention of education word but experience 2+: rank 2\n",
    "* if Phd and master's: rank 2\n",
    "* if master's and not phd: rank 1\n",
    "* if bachelor and/or master: rank 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_strings = [\"Responsibilities include Phd or Master's\", \"experience require P.hd.\", \n",
    "                \"Miss Must like a master\", \"do you have masters or P.hd?\",\n",
    "                \"5+ years experience with a phd in masters\", \"2 year experience in something\",\n",
    "                \"5 year experience with master's and phd.\", \"requires M.S. or Phd. data science\", \"I have a master's data\",\n",
    "                \"I can python your data!\", \"I'm a scientist.\", \"I'm a data!\", \"I can science ok\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "responsibilities include phd or master's\n",
      "0\n",
      "False True True False False False False\n",
      "\n",
      "\n",
      "experience require p.hd.\n",
      "-2\n",
      "False False True False False False False\n",
      "\n",
      "\n",
      "miss must like a master\n",
      "2\n",
      "False True False False False False False\n",
      "\n",
      "\n",
      "do you have masters or p.hd?\n",
      "0\n",
      "False True True False False False False\n",
      "\n",
      "\n",
      "5+ years experience with a phd in masters\n",
      "0\n",
      "False True True False False False False\n",
      "\n",
      "\n",
      "2 year experience in something\n",
      "0\n",
      "False False False False False False False\n",
      "\n",
      "\n",
      "5 year experience with master's and phd.\n",
      "0\n",
      "False True True False False False False\n",
      "\n",
      "\n",
      "requires m.s. or phd. data science\n",
      "2\n",
      "False True True False False False True\n",
      "\n",
      "\n",
      "i have a master's data\n",
      "2\n",
      "False True False False False False False\n",
      "\n",
      "\n",
      "i can python your data!\n",
      "2\n",
      "False False False False False False True\n",
      "\n",
      "\n",
      "i'm a scientist.\n",
      "0\n",
      "False False False False False False False\n",
      "\n",
      "\n",
      "i'm a data!\n",
      "0\n",
      "False False False False False False False\n",
      "\n",
      "\n",
      "i can science ok\n",
      "0\n",
      "False False False False False False False\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0, -2, 2, 0, 0, 0, 0, 2, 2, 2, 0, 0, 0]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranks = [0]*len(test_strings)\n",
    "words_of_interest = \"python|data science|data scientist|quantitative|analytical\"\n",
    "\n",
    "for i,test_string in enumerate(test_strings):\n",
    "    lowered = test_string.lower()\n",
    "    bachelor = True if re.search(\"bachelor|b\\.?s\\.?\", lowered) else False\n",
    "    master = True if re.search(\"master|ms|m\\.s\\.|ms\\.\", lowered) else False\n",
    "    phd = True if re.search(\"doctorate|p\\.?h\\.?d\\.?\", lowered) else False\n",
    "    experience_2 = True if re.search(\"2\\+?year[s]?\", lowered) else False\n",
    "    experience_5 = True if re.search(\"5\\+?year[s]?\", lowered) else False\n",
    "    experience_10 = True if re.search(\"10\\+?year[s]?\", lowered) else False\n",
    "    good_words = True if re.search(words_of_interest, lowered) else False\n",
    "    \n",
    "    if (experience_5 and phd) or (phd and not master) or (experience_10):\n",
    "        ranks[i] += -2\n",
    "    elif (experience_2 and not phd and not master) or (experience_5 and not phd):\n",
    "        ranks[i] += -1\n",
    "    elif (master and not phd and not experience_10) or (bachelor):\n",
    "        ranks[i] += 2\n",
    "    if good_words:\n",
    "        ranks[i] += 2\n",
    "        \n",
    "    print(lowered)\n",
    "    print(ranks[i])\n",
    "    print(bachelor,master,phd,experience_2,experience_5,experience_10,good_words)\n",
    "    print(\"\\n\")\n",
    "        \n",
    "ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a 3\n",
      "c 2\n",
      "b 1\n"
     ]
    }
   ],
   "source": [
    "test = ['a','a','c','b','c','a']\n",
    "\n",
    "counted = Counter(test).most_common()\n",
    "\n",
    "for count in counted:\n",
    "    print(count[0], count[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hi'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = ('hi', 'there')\n",
    "\n",
    "test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Responsibilities', 'include', 'Phd')"
      ]
     },
     "execution_count": 406,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigrams[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Responsibilities',\n",
       " 'include',\n",
       " 'Phd',\n",
       " 'Masters',\n",
       " 'Responsibilities include',\n",
       " 'include Phd',\n",
       " 'Phd Masters',\n",
       " 'Responsibilities include Phd',\n",
       " 'include Phd Masters']"
      ]
     },
     "execution_count": 410,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Masters', 1),\n",
       " ('include', 1),\n",
       " ('Phd Masters', 1),\n",
       " ('Responsibilities include Phd', 1),\n",
       " ('include Phd', 1),\n",
       " ('Responsibilities include', 1),\n",
       " ('Phd', 1),\n",
       " ('include Phd Masters', 1),\n",
       " ('Responsibilities', 1)]"
      ]
     },
     "execution_count": 411,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['?', 3, 1, '?', '?', '?', '?', '?', 1]\n"
     ]
    }
   ],
   "source": [
    "ranks = []\n",
    "\n",
    "for string in test_strings:\n",
    "    lowered = string.lower()\n",
    "    bachelor = True if re.search(\"bachelor|b\\.?s\\.?\", lowered) else False\n",
    "    master = True if re.search(\"master|ms|m\\.s\\.|ms\\.\", lowered) else False\n",
    "    phd = True if re.search(\"doctorate|p\\.?h\\.?d\\.?\", lowered) else False\n",
    "    experience_2 = True if re.search(\"2\\+?year[s]?\", lowered) else False\n",
    "    experience_5 = True if re.search(\"5\\+?year[s]?\", lowered) else False\n",
    "    experience_10 = True if re.search(\"10\\+?year[s]?\", lowered) else False\n",
    "    \n",
    "    if (experience_5 and phd) or (phd and not master) or (experience_10):\n",
    "        ranks.append(3)\n",
    "    elif (experience_2 and not phd and not master) or (experience_5 and not master):\n",
    "        ranks.append(2)\n",
    "    elif (master and not phd) or (bachelor) or ():\n",
    "        ranks.append(1)\n",
    "    else:\n",
    "        ranks.append(\"?\")\n",
    "        \n",
    "print(ranks)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parse the job descriptions for key word counts**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('YapStone', 'Data Scientist')\n",
      "('eBay Inc.', 'Sr. Data Scientist')\n",
      "('Reali', 'Product Manager')\n",
      "('eBay Inc.', 'Data Scientist, Marketing')\n",
      "('Esri', 'Data Scientist')\n",
      "('Axia Technologies', 'Senior Product Manager')\n",
      "('Trianz', 'Python Full Stack Developer')\n",
      "('realtor.com', 'Sr / Staff Software Engineer')\n",
      "('Entytle', '\\r\\n\\r\\n                        Data Analyst\\r\\n')\n",
      "('Match', 'Growth Marketer (Growth Hacker)')\n",
      "('Ancestry', 'Quality Engineer - Data Science Engineering Team')\n",
      "('Esri', 'Data Science Product Engineer - ArcGIS API for Python')\n",
      "('eBay Inc.', 'Staff Data Scientist')\n",
      "('M Theory Solutions', 'Data Scientist / Predictive Modeler')\n",
      "('eBay Inc.', 'Data Scientist/Applied Researcher')\n",
      "('Accenture', 'Data Scientist Associate Principal')\n",
      "('Metromile', 'Director of Analytics')\n",
      "('eBay Inc.', 'Senior Data Scientist - GBH')\n",
      "('Metromile', 'Senior Product Manager, Post-Purchase')\n",
      "('eBay Inc.', 'Data Scientist 2')\n",
      "Counter({'No': 519, 'Yes': 386})\n"
     ]
    }
   ],
   "source": [
    "df_test = compare_days(df_old, df_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0.0: 6,\n",
       "         1.0: 3,\n",
       "         4.0: 10,\n",
       "         5.0: 1,\n",
       "         6.0: 4,\n",
       "         7.0: 9,\n",
       "         8.0: 19,\n",
       "         9.0: 12,\n",
       "         10.0: 26,\n",
       "         11.0: 4,\n",
       "         12.0: 3,\n",
       "         13.0: 6,\n",
       "         14.0: 9,\n",
       "         15.0: 20,\n",
       "         16.0: 19,\n",
       "         17.0: 22,\n",
       "         18.0: 7,\n",
       "         20.0: 1,\n",
       "         21.0: 14,\n",
       "         22.0: 18,\n",
       "         23.0: 6,\n",
       "         24.0: 12,\n",
       "         25.0: 5,\n",
       "         27.0: 4,\n",
       "         28.0: 9})"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(df_test[(df_test['time'].notnull()) & (df_test['New']==\"Yes\")]['time'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
